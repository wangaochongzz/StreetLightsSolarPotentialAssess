{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.将一天的sungap当作一个月的sungap(sl_id,date,hour,sundur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:   0%|          | 0/30 [00:00<?, ?ID/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs: 100%|██████████| 30/30 [00:02<00:00, 12.00ID/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到：C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取数据文件到 DataFrame\n",
    "input_file = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\sungap.txt'\n",
    "data = pd.read_csv(input_file, sep=',', header=None, names=['id', '日期', '小时', '日照时长'])\n",
    "\n",
    "# 将日期列转换为datetime类型\n",
    "data['日期'] = pd.to_datetime(data['日期'])\n",
    "\n",
    "# 创建一个存储结果数据的列表\n",
    "result_data = []\n",
    "\n",
    "# 遍历每个id的分组\n",
    "for id, group in tqdm(data.groupby('id'), desc=\"Processing IDs\", unit=\"ID\"):\n",
    "    # 遍历每个月的数据\n",
    "    for date, sub_group in group.groupby(group['日期'].dt.to_period('M')):\n",
    "        # 获取每个月15号的数据\n",
    "        month_data = sub_group.copy()\n",
    "        month_date = month_data['日期'].iloc[0]\n",
    "\n",
    "        # 生成该月的每一天的日期\n",
    "        days_in_month = pd.date_range(month_date.replace(day=1), month_date + pd.offsets.MonthEnd(0))\n",
    "        \n",
    "        # 复制15号的数据，作为该月整月的数据\n",
    "        replicated_data = pd.concat([month_data.assign(日期=day) for day in days_in_month])\n",
    "        \n",
    "        # 添加到结果数据中\n",
    "        result_data.append(replicated_data)\n",
    "\n",
    "# 合并所有结果数据\n",
    "final_data = pd.concat(result_data, ignore_index=True)\n",
    "\n",
    "# 将合并后的数据保存为TXT文件\n",
    "output_file = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear.csv'\n",
    "final_data.to_csv(output_file, header=None, index=False, sep=',')\n",
    "\n",
    "# 打印提示信息\n",
    "print(f\"处理后的数据已保存到：{output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.在尾部添加上气象站真实总辐射值、直接辐射、散射辐射、天顶角\n",
    "# (sl_id,date,hour,sundur,GHI,DHI,DIR,Zenith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功写入到 C:\\Users\\wac\\Desktop\\yanzheng1\\results\\02_radiation_allyear.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\02_radiation.csv'\n",
    "data = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "# 确保数据有4745行\n",
    "if len(data) != 4745:\n",
    "    raise ValueError(f\"CSV文件的行数({len(data)})与预期的4745不符\")\n",
    "\n",
    "#这里乘的是points的数量\n",
    "multiplied_data = pd.concat([data] * 30, ignore_index=True)\n",
    "\n",
    "# 写入新的CSV文件\n",
    "output_file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\02_radiation_allyear.csv'\n",
    "multiplied_data.to_csv(output_file_path, index=False, header=False)\n",
    "\n",
    "print(f\"数据已成功写入到 {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging data: 100%|██████████| 4/4 [00:00<00:00, 1000.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并完成，并已保存到: C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear_radiation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 文件路径\n",
    "csv1_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear.csv'\n",
    "csv2_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\02_radiation_allyear.csv'\n",
    "output_csv_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear_radiation.csv'\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# 拼接到其他csv文件里\n",
    "# 读取第一个CSV文件\n",
    "df1 = pd.read_csv(csv1_path)\n",
    "\n",
    "# 读取第二个CSV文件\n",
    "df2 = pd.read_csv(csv2_path)\n",
    "\n",
    "# 合并两个DataFrame，使用 tqdm 显示进度条\n",
    "with tqdm(total=len(df2.columns), desc=\"Merging data\") as pbar:\n",
    "    merged_df = pd.concat([df1, df2], axis=1)\n",
    "    pbar.update(len(df2.columns))\n",
    "\n",
    "# 将合并后的数据写入新的CSV文件\n",
    "merged_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"合并完成，并已保存到:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.在尾部添加上散射占比um的值\n",
    "# # (sl_id,date,hour,sundur,GHI,DHI,DIR,Zenith,um)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\paper\\\\01_studyarea\\\\03_太阳能辐射估计\\\\03_02_跑全部的结果\\\\1_计算日照时长\\\\results\\\\0_所有街景图片的天空视域图_UM_SM.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38564\\3734608913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 读取Excel文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\1_计算日照时长\\results\\0_所有街景图片的天空视域图_UM_SM.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 创建保存CSV文件的目录\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\py37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\py37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32md:\\miniconda3\\envs\\py37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 )\n\u001b[0;32m   1194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\py37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     with get_handle(\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     ) as handle:\n\u001b[0;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\py37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\paper\\\\01_studyarea\\\\03_太阳能辐射估计\\\\03_02_跑全部的结果\\\\1_计算日照时长\\\\results\\\\0_所有街景图片的天空视域图_UM_SM.xlsx'"
     ]
    }
   ],
   "source": [
    "# 这个代码跳过，不运行\n",
    "import pandas as pd\n",
    "import os\n",
    "# 将汇总的UM_SM分成13份\n",
    "# 读取Excel文件\n",
    "file_path = r'C:\\Users\\wac\\Desktop\\yanzheng3\\img\\result\\0_所有街景图片的天空视域图_UM_SM.xlsx'\n",
    "df = pd.read_excel(file_path, header=None, names=['ID', 'UM'])\n",
    "\n",
    "# 创建保存CSV文件的目录\n",
    "output_dir =r'0_所有街景图片的天空视域图_UM_SM'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 定义每个CSV文件的数据范围\n",
    "chunk_size = 1000\n",
    "\n",
    "# 分割数据并保存为多个CSV文件\n",
    "for i in range(13):\n",
    "    start_id = i * chunk_size + 1\n",
    "    end_id = (i + 1) * chunk_size\n",
    "    if i == 12:  # 最后一部分\n",
    "        chunk_df = df[df['ID'] >= start_id]\n",
    "    else:\n",
    "        chunk_df = df[(df['ID'] >= start_id) & (df['ID'] <= end_id)]\n",
    "    \n",
    "    # 保存到CSV文件\n",
    "    chunk_df.to_csv(os.path.join(output_dir, f'chunk_{i+1}.csv'), index=False)\n",
    "\n",
    "print(\"完成分割并保存为CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成扩展并保存为新的CSV文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 每个id对应得UM值，扩大为一年得值，也就是4745倍\n",
    "# 读取CSV文件，假设第二列是目标数据\n",
    "input_file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\03_um1.csv'\n",
    "  # 请替换为你的CSV文件路径\n",
    "df = pd.read_csv(input_file_path, header=None, usecols=[1], names=['UM'])\n",
    "\n",
    "# 初始化一个空的DataFrame用于存储扩展的数据\n",
    "expanded_df = pd.DataFrame()\n",
    "\n",
    "# 将第二列的每一行数据复制为4745个相同值\n",
    "for value in df['UM']:\n",
    "    repeated_values = [value] * 4745\n",
    "    expanded_df = pd.concat([expanded_df, pd.DataFrame(repeated_values, columns=['UM'])], ignore_index=True)\n",
    "\n",
    "# 保存到新的CSV文件，不包含header\n",
    "output_file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\03_um1_allyear.csv'\n",
    "expanded_df.to_csv(output_file_path, index=False, header=False)\n",
    "\n",
    "print(\"完成扩展并保存为新的CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV文件的行数为: 4744\n"
     ]
    }
   ],
   "source": [
    "# 这个代码跳过，不运行\n",
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = r'C:\\Users\\wac\\Desktop\\yanzheng3\\img\\result\\01_results1_allyear_radiation.csv'\n",
    "\n",
    "try:\n",
    "    # 分块读取CSV文件\n",
    "    chunk_size = 10000  # 每次读取10000行\n",
    "    num_rows = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        num_rows += len(chunk)\n",
    "    \n",
    "    print(f\"CSV文件的行数为: {num_rows}\")\n",
    "except Exception as e:\n",
    "    print(f\"读取CSV文件时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成合并并保存为新的CSV文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 将扩大后得um添加到合并csv文件中\n",
    "# 读取第一个CSV文件\n",
    "file1_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear_radiation.csv'  # 请替换为你的第一个CSV文件路径\n",
    "df1 = pd.read_csv(file1_path)\n",
    "\n",
    "# 读取第二个CSV文件\n",
    "file2_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\03_um1_allyear.csv' # 请替换为你的第二个CSV文件路径\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# 检查两个文件的行数是否相同\n",
    "if len(df1) != len(df2):\n",
    "    raise ValueError(\"两个CSV文件的行数不相同，无法合并。\")\n",
    "\n",
    "# 合并两个DataFrame\n",
    "merged_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# 保存合并后的CSV文件\n",
    "output_file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear_radiation_um.csv'  # 请替换为你要保存的CSV文件路径\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"完成合并并保存为新的CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.计算总辐射值、直接辐射值、散射辐射值\n",
    "# (sl_id,date,hour,sundur,GHI,DHI,DIR,Zenith,um,GHI_考虑遮挡,DHI_考虑遮挡,DIR_考虑遮挡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 142350/142350 [00:00<00:00, 193947.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算完成并保存为新的CSV文件。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 读取并处理CSV文件的函数\n",
    "def process_csv(file_path, output_file_path, chunksize=100000):\n",
    "    # 计算总行数\n",
    "    total_rows = sum(1 for _ in open(file_path))\n",
    "    chunks = pd.read_csv(file_path, chunksize=chunksize, header=None)\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"Processing\") as pbar:\n",
    "        for chunk in chunks:\n",
    "            # 将角度从度转换为弧度\n",
    "            radians = np.radians(chunk.iloc[:, 7].astype(float))\n",
    "            # 计算余弦值\n",
    "            cos_values = np.cos(radians)\n",
    "            \n",
    "            # 计算第11列的值：=F5*D5*I5+G5*J5\n",
    "            chunk['J'] = chunk.iloc[:, 5] * chunk.iloc[:, 3] * cos_values + chunk.iloc[:, 6] * chunk.iloc[:, 8]\n",
    "            chunk['K'] = chunk.iloc[:, 5] * chunk.iloc[:, 3]\n",
    "            chunk['L'] = chunk.iloc[:, 6] * chunk.iloc[:, 8]\n",
    "            \n",
    "            # 追加到输出文件，不包含header\n",
    "            chunk.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "# 文件路径\n",
    "file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear_radiation_um.csv'  # 请替换为你的CSV文件路径\n",
    "output_file_path = r'C:\\Users\\wac\\Desktop\\yanzheng1\\results\\01_results1_allyear_radiation_um_predict1.csv'  # 请替换为你要保存的CSV文件路径\n",
    "\n",
    "# 处理CSV文件并显示进度\n",
    "process_csv(file_path, output_file_path)\n",
    "\n",
    "print(\"计算完成并保存为新的CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.计算太阳能电池板上的有效辐射值\n",
    "# (sl_id,date,hour,sundur,GHI,DHI,DIR,Zenith,um,GHI_考虑遮挡,DHI_考虑遮挡,DIR_考虑遮挡,GHI_电池板上倾斜表面)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 4745000/4745000 [00:26<00:00, 179249.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算完成并保存为新的CSV文件。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "# 读取并处理CSV文件的函数\n",
    "def process_csv(file_path, output_file_path, chunksize=100000):\n",
    "    # 计算总行数\n",
    "    total_rows = sum(1 for _ in open(file_path))\n",
    "    chunks = pd.read_csv(file_path, chunksize=chunksize, header=None)\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"Processing\") as pbar:\n",
    "        for chunk in chunks:\n",
    "            # 将角度从度转换为弧度\n",
    "            radians2 = np.radians((chunk.iloc[:, 7]-17).astype(float))\n",
    "            # 计算余弦值\n",
    "            cos_values2 = np.cos(radians2)\n",
    "            \n",
    "            # 计算第11列的值：=F5*D5*I5+G5*J5\n",
    "            chunk['M'] = 0.98*(chunk.iloc[:, 10]* cos_values2 + chunk.iloc[:, 9]*0.2*(1-math.cos(math.radians(17)))/2 + chunk.iloc[:, 11]*(1+math.cos(math.radians(17)))/2 + chunk.iloc[:, 9]*(0.012*(chunk.iloc[:, 7]-0.04))*(1-math.cos(math.radians(17)))/2)\n",
    "            # 追加到输出文件，不包含header\n",
    "\n",
    "            chunk.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "# 文件路径\n",
    "file_path =  r'C:\\Users\\wac\\Desktop\\yanzheng3\\img\\result\\01_results1_allyear_radiation_um_predict1.csv'  # 请替换为你的CSV文件路径\n",
    "output_file_path =  r'C:\\Users\\wac\\Desktop\\yanzheng3\\img\\result\\01_results1_allyear_radiation_um_predict2.csv'  # 请替换为你要保存的CSV文件路径\n",
    "\n",
    "# 处理CSV文件并显示进度\n",
    "process_csv(file_path, output_file_path)\n",
    "\n",
    "print(\"计算完成并保存为新的CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 计算日均辐射值和逐小时辐射值\n",
    "# (sl_id,date,hour,sundur,GHI,DHI,DIR,Zenith,um,GHI_考虑遮挡,DHI_考虑遮挡,DIR_考虑遮挡,GHI_电池板上倾斜表面)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取文件\n",
    "file_path =  r'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\right.csv'\n",
    "\n",
    "# 读取数据到DataFrame\n",
    "data = pd.read_csv(file_path, header=None, names=['sl_id', 'date', 'hour', 'sundur','GHI','DHI','DIF','Zenith','um', 'GHI_hor', 'DHI_hor', 'DIF_hor','GHI_pv'])\n",
    "\n",
    "# 第一部分：计算日均辐射值、日照时长\n",
    "result1 = data.groupby('sl_id').agg({'sundur': 'sum', 'GHI_hor': 'sum', 'DHI_hor': 'sum', 'DIF_hor': 'sum','GHI_pv':'sum'}) / 365\n",
    "\n",
    "# 第二部分：计算小时平均辐射值\n",
    "result_df = pd.DataFrame(columns=['sl_id', '6_hor', '7_hor', '8_hor', '9_hor', '10_hor', '11_hor', '12_hor', '13_hor', '14_hor', '15_hor', '16_hor', '17_hor', '18_hor','6_pv', '7_pv', '8_pv', '9_pv', '10_pv', '11_pv', '12_pv', '13_pv', '14_pv', '15_pv', '16_pv', '17_pv', '18_pv'])\n",
    "\n",
    "for id_value in data['sl_id'].unique():\n",
    "    id_data = data[data['sl_id'] == id_value]\n",
    "    avg_radiation_per_hour = []\n",
    "    avg_radiation_per_hour2 = []\n",
    "    for hour in range(6, 19):\n",
    "        avg_radiation = id_data[(id_data['hour'] == hour)]['GHI_hor'].mean()\n",
    "        avg_radiation2 = id_data[(id_data['hour'] == hour)]['GHI_pv'].mean()\n",
    "        avg_radiation_per_hour.append(avg_radiation)\n",
    "        avg_radiation_per_hour2.append(avg_radiation2)\n",
    "    result_df = result_df.append({'sl_id': id_value, '6_hor': avg_radiation_per_hour[0], '7_hor': avg_radiation_per_hour[1], \n",
    "                                  '8_hor': avg_radiation_per_hour[2], '9_hor': avg_radiation_per_hour[3], '10_hor': avg_radiation_per_hour[4], \n",
    "                                  '11_hor': avg_radiation_per_hour[5], '12_hor': avg_radiation_per_hour[6], '13_hor': avg_radiation_per_hour[7], \n",
    "                                  '14_hor': avg_radiation_per_hour[8], '15_hor': avg_radiation_per_hour[9], '16_hor': avg_radiation_per_hour[10], \n",
    "                                  '17_hor': avg_radiation_per_hour[11], '18_hor': avg_radiation_per_hour[12],'6_pv': avg_radiation_per_hour2[0], '7_pv': avg_radiation_per_hour2[1], \n",
    "                                  '8_pv': avg_radiation_per_hour2[2], '9_pv': avg_radiation_per_hour2[3], '10_pv': avg_radiation_per_hour2[4], \n",
    "                                  '11_pv': avg_radiation_per_hour2[5], '12_pv': avg_radiation_per_hour2[6], '13_pv': avg_radiation_per_hour2[7], \n",
    "                                  '14_pv': avg_radiation_per_hour2[8], '15_pv': avg_radiation_per_hour2[9], '16_pv': avg_radiation_per_hour2[10], \n",
    "                                  '17_pv': avg_radiation_per_hour2[11], '18_pv': avg_radiation_per_hour2[12]}, \n",
    "                                 ignore_index=True)\n",
    "\n",
    "# 合并结果并保存到一个CSV文件中\n",
    "result_merged = pd.merge(result1, result_df, on='sl_id')\n",
    "result_merged.to_csv( r'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\right_average.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.合并所有csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 148.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的文件已保存至 D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\average\\combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # 正确导入 tqdm 库中的 tqdm 函数\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = r'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\average'  # 替换为你的文件夹路径\n",
    "\n",
    "# 获取文件夹中所有的CSV文件名\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# 确保文件按照文件名的自然数顺序排序\n",
    "csv_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "# 初始化一个空的DataFrame来存储合并后的数据\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# 使用tqdm显示进度条\n",
    "with tqdm(total=len(csv_files)) as pbar:\n",
    "    # 逐个读取和合并CSV文件\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        df = pd.read_csv(file_path)  # 读取CSV文件\n",
    "        \n",
    "        # 合并到总的DataFrame中\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        \n",
    "        # 更新进度条\n",
    "        pbar.update(1)\n",
    "\n",
    "# 将合并后的DataFrame保存为一个新的CSV文件，只保留一个header\n",
    "output_file = os.path.join(folder_path, 'combined.csv')  # 输出文件路径为文件夹下的 combined.csv\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"合并后的文件已保存至 {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.替换掉个别不对的点的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "替换操作完成，结果已保存到'test2.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取第一个CSV文件，包含要替换的ID\n",
    "csv1 = pd.read_csv(r'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\right_average.csv', encoding='GBK',sep=',')\n",
    "\n",
    "# 读取第二个CSV文件，包含被替换的sl_id列\n",
    "csv2 = pd.read_csv(r'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\average\\combined.csv', encoding='GBK',sep=',')\n",
    "\n",
    "# 遍历csv1的每一行，替换csv2中对应的行\n",
    "for index, row in csv1.iterrows():\n",
    "    id_value = row['sl_id']\n",
    "    # 找到csv2中匹配sl_id的行，并用csv1的行数据替换它\n",
    "    csv2.loc[csv2['sl_id'] == id_value, csv2.columns] = row.values\n",
    "\n",
    "# 保存结果到一个新的CSV文件中\n",
    "csv2.to_csv(r'D:\\paper\\01_studyarea\\03_太阳能辐射估计\\03_02_跑全部的结果\\5_所有点\\folder_all\\average\\combined2.csv', index=False)\n",
    "\n",
    "print(\"替换操作完成，结果已保存到'test2.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
